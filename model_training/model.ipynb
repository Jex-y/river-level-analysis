{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_features)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 590641 entries, 2007-01-01 12:00:00 to 2023-11-06 00:00:00\n",
      "Columns: 324 entries, Level Chester Le Street to Rainfall Tunstall -7d\n",
      "dtypes: float16(324)\n",
      "memory usage: 369.5 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_feather('data/river_wear_lagged.feather')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "    col \n",
    "    for col \n",
    "    in df.columns \n",
    "    if col.startswith('Level Durham New Elvet Bridge +')\n",
    "]\n",
    "\n",
    "df['day_of_year'] = df.index.dayofyear\n",
    "df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "df = df.drop(columns=['day_of_year'])\n",
    "\n",
    "X = df.drop(targets, axis=1)\n",
    "y = df[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "\n",
    "# Normalize the data\n",
    "X_pipeline = ColumnTransformer(\n",
    "    [\n",
    "        ('Normalise level and flow', preprocessing.StandardScaler(), make_column_selector(pattern='Level|Flow')),\n",
    "        ('Normalise rainfall', preprocessing.MinMaxScaler(), make_column_selector(pattern='Rainfall')),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X = X_pipeline.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pipeline = preprocessing.StandardScaler()\n",
    "\n",
    "y = y_pipeline.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\torch\\lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: overflow encountered in cast\n",
      "  scl = avg_as_array.dtype.type(a.size/avg_as_array.size)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9358188895852896"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     torch.utils.data.TensorDataset(\n",
    "#         torch.tensor(X_train, dtype=torch.float32), \n",
    "#         torch.tensor(y_train, dtype=torch.float32)\n",
    "#     ), \n",
    "#     batch_size=8192,\n",
    "#     num_workers=4\n",
    "# )\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     torch.utils.data.TensorDataset(\n",
    "#         torch.tensor(X_val, dtype=torch.float32), \n",
    "#         torch.tensor(y_val, dtype=torch.float32)\n",
    "#     ), \n",
    "#     batch_size=8192,\n",
    "#     num_workers=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Model(X_train.shape[1], y_train.shape[1]).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "target_time_offsets = list(range(15, 12*60+15, 15))\n",
    "\n",
    "metrics = {\n",
    "   f\"RMSE +{offset} mins\": lambda y_pred, y_true: torch.sqrt(criterion(y_pred[:,i], y_true[:, i]))\n",
    "    for i, offset\n",
    "    in enumerate(target_time_offsets)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ed\\AppData\\Local\\Temp\\ipykernel_1612\\4148899208.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "def validate_model(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_metrics = {k: 0 for k in metrics}\n",
    "        # for x, y in test_loader:\n",
    "        #     x, y = x.to(device), y.to(device)\n",
    "        #     y_pred = model(x)\n",
    "        #     val_loss += criterion(y_pred, y).item()\n",
    "        #     for k in metrics:\n",
    "        #         val_metrics[k] += metrics[k](y_pred, y).item()\n",
    "                \n",
    "        # val_loss /= len(test_loader)\n",
    "        # for k in metrics:\n",
    "        #     val_metrics[k] /= len(test_loader)\n",
    "        x,y = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        val_loss = criterion(y_pred, y).item()\n",
    "        for k in metrics:\n",
    "            val_metrics[k] = metrics[k](y_pred, y).item()\n",
    "            \n",
    "        wandb.log(\n",
    "            {\n",
    "                'val_loss': val_loss, \n",
    "                **{'val_' + k: v for k, v in val_metrics.items()}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7lrs1yvw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c357787526c4106bd71842e8b760d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.015 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.088945…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>▁█</td></tr><tr><td>val_RMSE +105 mins</td><td>▁</td></tr><tr><td>val_RMSE +120 mins</td><td>▁</td></tr><tr><td>val_RMSE +135 mins</td><td>▁</td></tr><tr><td>val_RMSE +15 mins</td><td>▁</td></tr><tr><td>val_RMSE +150 mins</td><td>▁</td></tr><tr><td>val_RMSE +165 mins</td><td>▁</td></tr><tr><td>val_RMSE +180 mins</td><td>▁</td></tr><tr><td>val_RMSE +195 mins</td><td>▁</td></tr><tr><td>val_RMSE +210 mins</td><td>▁</td></tr><tr><td>val_RMSE +225 mins</td><td>▁</td></tr><tr><td>val_RMSE +240 mins</td><td>▁</td></tr><tr><td>val_RMSE +255 mins</td><td>▁</td></tr><tr><td>val_RMSE +270 mins</td><td>▁</td></tr><tr><td>val_RMSE +285 mins</td><td>▁</td></tr><tr><td>val_RMSE +30 mins</td><td>▁</td></tr><tr><td>val_RMSE +300 mins</td><td>▁</td></tr><tr><td>val_RMSE +315 mins</td><td>▁</td></tr><tr><td>val_RMSE +330 mins</td><td>▁</td></tr><tr><td>val_RMSE +345 mins</td><td>▁</td></tr><tr><td>val_RMSE +360 mins</td><td>▁</td></tr><tr><td>val_RMSE +375 mins</td><td>▁</td></tr><tr><td>val_RMSE +390 mins</td><td>▁</td></tr><tr><td>val_RMSE +405 mins</td><td>▁</td></tr><tr><td>val_RMSE +420 mins</td><td>▁</td></tr><tr><td>val_RMSE +435 mins</td><td>▁</td></tr><tr><td>val_RMSE +45 mins</td><td>▁</td></tr><tr><td>val_RMSE +450 mins</td><td>▁</td></tr><tr><td>val_RMSE +465 mins</td><td>▁</td></tr><tr><td>val_RMSE +480 mins</td><td>▁</td></tr><tr><td>val_RMSE +495 mins</td><td>▁</td></tr><tr><td>val_RMSE +510 mins</td><td>▁</td></tr><tr><td>val_RMSE +525 mins</td><td>▁</td></tr><tr><td>val_RMSE +540 mins</td><td>▁</td></tr><tr><td>val_RMSE +555 mins</td><td>▁</td></tr><tr><td>val_RMSE +570 mins</td><td>▁</td></tr><tr><td>val_RMSE +585 mins</td><td>▁</td></tr><tr><td>val_RMSE +60 mins</td><td>▁</td></tr><tr><td>val_RMSE +600 mins</td><td>▁</td></tr><tr><td>val_RMSE +615 mins</td><td>▁</td></tr><tr><td>val_RMSE +630 mins</td><td>▁</td></tr><tr><td>val_RMSE +645 mins</td><td>▁</td></tr><tr><td>val_RMSE +660 mins</td><td>▁</td></tr><tr><td>val_RMSE +675 mins</td><td>▁</td></tr><tr><td>val_RMSE +690 mins</td><td>▁</td></tr><tr><td>val_RMSE +705 mins</td><td>▁</td></tr><tr><td>val_RMSE +720 mins</td><td>▁</td></tr><tr><td>val_RMSE +75 mins</td><td>▁</td></tr><tr><td>val_RMSE +90 mins</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.12493</td></tr><tr><td>val_RMSE +105 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +120 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +135 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +15 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +150 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +165 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +180 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +195 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +210 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +225 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +240 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +255 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +270 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +285 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +30 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +300 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +315 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +330 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +345 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +360 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +375 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +390 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +405 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +420 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +435 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +45 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +450 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +465 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +480 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +495 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +510 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +525 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +540 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +555 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +570 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +585 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +60 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +600 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +615 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +630 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +645 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +660 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +675 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +690 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +705 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +720 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +75 mins</td><td>0.55245</td></tr><tr><td>val_RMSE +90 mins</td><td>0.55245</td></tr><tr><td>val_loss</td><td>0.13157</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-jazz-9</strong> at: <a href='https://wandb.ai/ejex/river-levels/runs/7lrs1yvw' target=\"_blank\">https://wandb.ai/ejex/river-levels/runs/7lrs1yvw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231113_213659-7lrs1yvw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7lrs1yvw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f984063f24d4575b9c14155d13e3de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\code\\river-level-analysis\\model_training\\wandb\\run-20231113_213936-prruikqp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ejex/river-levels/runs/prruikqp' target=\"_blank\">swept-snowflake-10</a></strong> to <a href='https://wandb.ai/ejex/river-levels' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ejex/river-levels' target=\"_blank\">https://wandb.ai/ejex/river-levels</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ejex/river-levels/runs/prruikqp' target=\"_blank\">https://wandb.ai/ejex/river-levels/runs/prruikqp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3637f9ab250b4295897d6ae2f0f17257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/100000 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\code\\river-level-analysis\\model_training\\model.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/river-level-analysis/model_training/model.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/river-level-analysis/model_training/model.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/code/river-level-analysis/model_training/model.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train_losses[i \u001b[39m%\u001b[39m train_loss_smoothing] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/river-level-analysis/model_training/model.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m train_loss_smoothing \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/river-level-analysis/model_training/model.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     wandb\u001b[39m.\u001b[39mlog({\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m: train_losses\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem()})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run = wandb.init(project='river-levels')\n",
    "\n",
    "# train_iter = cycle(train_loader)\n",
    "\n",
    "train_steps = 100_000\n",
    "val_freq = 1000\n",
    "train_loss_smoothing = 100\n",
    "\n",
    "train_losses = torch.zeros(train_loss_smoothing)\n",
    "x, y = X_train, y_train\n",
    "x, y = torch.tensor(x, dtype=torch.float32).to(device), torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "with tqdm(total=train_steps, desc=\"Train\", unit=\"batch\") as pbar:\n",
    "    for i in range(train_steps):\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses[i % train_loss_smoothing] = loss.item()\n",
    "        if i % train_loss_smoothing == 0:\n",
    "            wandb.log({'train_loss': train_losses.mean().item()})\n",
    "            pbar.set_postfix({'train_loss': train_losses.mean().item()})\n",
    "            pbar.update(train_loss_smoothing)\n",
    "        \n",
    "        # if i % val_freq == 0:\n",
    "        #     val_loss = validate_model(model)\n",
    "        #     model.train()\n",
    "        #     pbar.set_postfix({'val_loss': val_loss,})\n",
    "        #     pbar.update(val_freq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "with open('X_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(X_pipeline, f)\n",
    "with open('y_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pipeline, f)\n",
    "    \n",
    "run.log_artifact('model.pt')\n",
    "run.log_artifact('X_pipeline.pkl')\n",
    "run.log_artifact('y_pipeline.pkl')\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "river-level",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

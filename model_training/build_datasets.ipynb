{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ujson'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, HTML, Markdown\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhydrology_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HydrologyApi, Measure, process_hydrology_data\n",
      "File \u001b[1;32md:\\code\\river-level-analysis\\model_training\\hydrology_api\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhydrology_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Measure, HydrologyApi, process_hydrology_data\n",
      "File \u001b[1;32md:\\code\\river-level-analysis\\model_training\\hydrology_api\\hydrology_api.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIO\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mujson\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ujson'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from hydrology_api import HydrologyApi, Measure, process_hydrology_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HydrologyApi(max_threads=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 5 stations: ['Chester Le Street' 'Witton Park' 'Sunderland Bridge' 'Stanhope'\n",
      " 'Durham New Elvet Bridge']\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=05784319-693a-4d75-b29e-32f01a99ee4f-level-i-900-m-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=e7d8bbb6-5bba-4057-9f49-a299482c3348-level-i-900-m-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=ddedb4d9-b2be-47c1-998d-acbc0ffb124b-level-i-900-m-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=b29c481a-5012-40f5-bb0c-f9370be34975-level-i-900-m-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=ba3f8598-e654-430d-9bb8-e1652e6ff93d-level-i-900-m-qualified&mineq-date=2007-01-01\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 601006 entries, 2007-01-01 00:00:00 to 2024-02-21 11:15:00\n",
      "Freq: 15min\n",
      "Data columns (total 5 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Chester Le Street        601006 non-null  float16\n",
      " 1   Durham New Elvet Bridge  573865 non-null  float16\n",
      " 2   Stanhope                 511654 non-null  float16\n",
      " 3   Sunderland Bridge        599457 non-null  float16\n",
      " 4   Witton Park              595206 non-null  float16\n",
      "dtypes: float16(5)\n",
      "memory usage: 10.3 MB\n"
     ]
    }
   ],
   "source": [
    "stations = api.get_stations_on_river('River Wear', Measure.LEVEL)\n",
    "print(f\"Loading data for {len(stations)} stations: {stations['label'].values}\")\n",
    "level_df = api.load(Measure.LEVEL, stations)\n",
    "level_df = process_hydrology_data(level_df)\n",
    "level_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Flow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 4 stations: ['Chester Le Street' 'Witton Park' 'Sunderland Bridge' 'Stanhope']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 592345 entries, 2007-01-01 00:00:00 to 2023-11-23 06:00:00\n",
      "Freq: 15min\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   Chester Le Street  592345 non-null  float16\n",
      " 1   Stanhope           502991 non-null  float16\n",
      " 2   Sunderland Bridge  590796 non-null  float16\n",
      " 3   Witton Park        586545 non-null  float16\n",
      "dtypes: float16(4)\n",
      "memory usage: 9.0 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=e7d8bbb6-5bba-4057-9f49-a299482c3348-flow-i-900-m3s-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=ddedb4d9-b2be-47c1-998d-acbc0ffb124b-flow-i-900-m3s-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=05784319-693a-4d75-b29e-32f01a99ee4f-flow-i-900-m3s-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=b29c481a-5012-40f5-bb0c-f9370be34975-flow-i-900-m3s-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=bc34e640-d9ae-4362-8804-25d66ca66e4d-rainfall-t-900-mm-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=1dabd12c-1d2e-4765-ae38-a4d5a121928d-rainfall-t-900-mm-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=bf61ce31-b20e-4593-85dc-a083133b12ce-rainfall-t-900-mm-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=051f1b2a-6aca-4402-8956-5474ad39b12a-rainfall-t-900-mm-qualified&mineq-date=2007-01-01\n",
      "Making request to: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=a8773476-0fde-40c7-a66b-0901e528e8f2-rainfall-t-900-mm-qualified&mineq-date=2007-01-01\n"
     ]
    }
   ],
   "source": [
    "stations = api.get_stations_on_river('River Wear', Measure.FLOW)\n",
    "print(f\"Loading data for {len(stations)} stations: {stations['label'].values}\")\n",
    "flow_df = api.load(Measure.FLOW, stations)\n",
    "flow_df = process_hydrology_data(flow_df)\n",
    "flow_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have flow data at New Elvet unfortunatly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Rainfall Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5 rainfall stations: ['Evenwood Gate' 'Harpington Hill Farm' 'Copley' 'Tunstall'\n",
      " 'Darlington Lingfield Way']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 601006 entries, 2007-01-01 00:00:00 to 2024-02-21 11:15:00\n",
      "Freq: 15min\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   Copley                    586404 non-null  float16\n",
      " 1   Darlington Lingfield Way  552660 non-null  float16\n",
      " 2   Evenwood Gate             527518 non-null  float16\n",
      " 3   Harpington Hill Farm      533221 non-null  float16\n",
      " 4   Tunstall                  556093 non-null  float16\n",
      "dtypes: float16(5)\n",
      "memory usage: 10.3 MB\n"
     ]
    }
   ],
   "source": [
    "rainfall_stations = api.get_stations_close_to_with_measure(54.66305556, -1.67611111, 15, Measure.RAINFALL, limit=10)\n",
    "bad_stations = ['15202aee-c5fd-404d-9de9-7357174ad10c']\n",
    "rainfall_stations = rainfall_stations[~rainfall_stations['notation'].isin(bad_stations)].head(5)\n",
    "print(f\"Using {len(rainfall_stations)} rainfall stations: {rainfall_stations['label'].values}\")\n",
    "\n",
    "rainfall_df = api.load(Measure.RAINFALL, rainfall_stations)\n",
    "rainfall_df = process_hydrology_data(rainfall_df)\n",
    "rainfall_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 601006 entries, 2007-01-01 00:00:00 to 2024-02-21 11:15:00\n",
      "Freq: 15min\n",
      "Data columns (total 14 columns):\n",
      " #   Column                             Non-Null Count   Dtype  \n",
      "---  ------                             --------------   -----  \n",
      " 0   Level Chester Le Street            601006 non-null  float16\n",
      " 1   Level Durham New Elvet Bridge      573865 non-null  float16\n",
      " 2   Level Stanhope                     511654 non-null  float16\n",
      " 3   Level Sunderland Bridge            599457 non-null  float16\n",
      " 4   Level Witton Park                  595206 non-null  float16\n",
      " 5   Flow Chester Le Street             592345 non-null  float16\n",
      " 6   Flow Stanhope                      502991 non-null  float16\n",
      " 7   Flow Sunderland Bridge             590796 non-null  float16\n",
      " 8   Flow Witton Park                   586545 non-null  float16\n",
      " 9   Rainfall Copley                    586404 non-null  float16\n",
      " 10  Rainfall Darlington Lingfield Way  552660 non-null  float16\n",
      " 11  Rainfall Evenwood Gate             527518 non-null  float16\n",
      " 12  Rainfall Harpington Hill Farm      533221 non-null  float16\n",
      " 13  Rainfall Tunstall                  556093 non-null  float16\n",
      "dtypes: float16(14)\n",
      "memory usage: 20.6 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(\n",
    "    level_df.add_prefix('Level '),\n",
    "    flow_df.add_prefix('Flow '),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='outer',\n",
    ")\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    rainfall_df.add_prefix('Rainfall '),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='outer',\n",
    ")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 601006 entries, 2007-01-01 00:00:00 to 2024-02-21 11:15:00\n",
      "Freq: 15min\n",
      "Data columns (total 14 columns):\n",
      " #   Column                             Non-Null Count   Dtype  \n",
      "---  ------                             --------------   -----  \n",
      " 0   Level Chester Le Street            601006 non-null  float16\n",
      " 1   Level Durham New Elvet Bridge      601006 non-null  float16\n",
      " 2   Level Stanhope                     601006 non-null  float16\n",
      " 3   Level Sunderland Bridge            601006 non-null  float16\n",
      " 4   Level Witton Park                  601006 non-null  float16\n",
      " 5   Flow Chester Le Street             601006 non-null  float16\n",
      " 6   Flow Stanhope                      601006 non-null  float16\n",
      " 7   Flow Sunderland Bridge             601006 non-null  float16\n",
      " 8   Flow Witton Park                   601006 non-null  float16\n",
      " 9   Rainfall Copley                    601006 non-null  float16\n",
      " 10  Rainfall Darlington Lingfield Way  601006 non-null  float16\n",
      " 11  Rainfall Evenwood Gate             601006 non-null  float16\n",
      " 12  Rainfall Harpington Hill Farm      601006 non-null  float16\n",
      " 13  Rainfall Tunstall                  601006 non-null  float16\n",
      "dtypes: float16(14)\n",
      "memory usage: 20.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Fill na on flow and level by linear interpolation\n",
    "flow_and_level_cols = [col for col in df.columns if col.startswith('Flow') or col.startswith('Level')]\n",
    "df[flow_and_level_cols] = df[flow_and_level_cols].interpolate('time')\n",
    "\n",
    "rainfall_cols = [col for col in df.columns if col.startswith('Rainfall')]\n",
    "df[rainfall_cols] = df[rainfall_cols].fillna(0)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning.core.lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TemporalFusionTransformer, TimeSeriesDataSet\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GroupNormalizer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MAE, SMAPE, PoissonLoss, QuantileLoss\n",
      "File \u001b[0;32m~/micromamba/envs/river/lib/python3.10/site-packages/pytorch_forecasting/__init__.py:31\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     EncoderNormalizer,\n\u001b[1;32m      6\u001b[0m     GroupNormalizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     TimeSeriesDataSet,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     MAE,\n\u001b[1;32m     13\u001b[0m     MAPE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     QuantileLoss,\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     GRU,\n\u001b[1;32m     33\u001b[0m     LSTM,\n\u001b[1;32m     34\u001b[0m     AutoRegressiveBaseModel,\n\u001b[1;32m     35\u001b[0m     AutoRegressiveBaseModelWithCovariates,\n\u001b[1;32m     36\u001b[0m     Baseline,\n\u001b[1;32m     37\u001b[0m     BaseModel,\n\u001b[1;32m     38\u001b[0m     BaseModelWithCovariates,\n\u001b[1;32m     39\u001b[0m     DecoderMLP,\n\u001b[1;32m     40\u001b[0m     DeepAR,\n\u001b[1;32m     41\u001b[0m     MultiEmbedding,\n\u001b[1;32m     42\u001b[0m     NBeats,\n\u001b[1;32m     43\u001b[0m     NHiTS,\n\u001b[1;32m     44\u001b[0m     RecurrentNetwork,\n\u001b[1;32m     45\u001b[0m     TemporalFusionTransformer,\n\u001b[1;32m     46\u001b[0m     get_rnn,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     49\u001b[0m     apply_to_list,\n\u001b[1;32m     50\u001b[0m     autocorrelation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     unpack_sequence,\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     62\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeSeriesDataSet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroupNormalizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munpack_sequence\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m ]\n",
      "File \u001b[0;32m~/micromamba/envs/river/lib/python3.10/site-packages/pytorch_forecasting/models/__init__.py:11\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     AutoRegressiveBaseModel,\n\u001b[1;32m      6\u001b[0m     AutoRegressiveBaseModelWithCovariates,\n\u001b[1;32m      7\u001b[0m     BaseModel,\n\u001b[1;32m      8\u001b[0m     BaseModelWithCovariates,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbaseline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Baseline\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepAR\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecoderMLP\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnbeats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NBeats\n",
      "File \u001b[0;32m~/micromamba/envs/river/lib/python3.10/site-packages/pytorch_forecasting/models/deepar/__init__.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningModule\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdists\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning.core.lightning'"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import (\n",
    "    optimize_hyperparameters,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rainfall_hourly = df.filter(regex='Rainfall').resample('1h').sum().resample('15min').interpolate('time')\n",
    "df_rainfall_six_hourly = df.filter(regex='Rainfall').resample('6h').sum().resample('15min').interpolate('time')\n",
    "df_rainfall_daily = df.filter(regex='Rainfall').resample('1d').sum().resample('15min').interpolate('time')\n",
    "\n",
    "target_cols = ['Level Durham New Elvet Bridge']\n",
    "target_shifts = range(15, 12*60 + 15, 15)\n",
    "level_shifts = [-15, -30, -60, -90, -120, -180, -240, -300, -360, -420, -480, -540, -600, -660, -720, -780, -840, -900]\n",
    "flow_shifts = level_shifts\n",
    "rainfall_min_shifts = [-15, -30, -60]\n",
    "rainfall_hour_shifts = [-2, -3, -4, -5, -6]\n",
    "rainfall_six_hour_shifts = [-12, -18, -24, -30, -36, -42, -48]\n",
    "rainfall_day_shifts = [-3, -4, -5, -6, -7]\n",
    "\n",
    "output_target_cols = []\n",
    "\n",
    "df_lagged = df.copy()\n",
    "\n",
    "for shift in target_shifts:\n",
    "    shifted_df = df[target_cols].shift(shift, freq='min')\n",
    "    shifted_df = shifted_df.add_suffix(f' {shift:+d}min')\n",
    "    output_target_cols.extend(shifted_df.columns)\n",
    "    df_lagged = pd.merge(df_lagged, shifted_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "for shift in level_shifts:\n",
    "    shifted_df = df.filter(regex='Level').shift(shift, freq='min')\n",
    "    df_lagged = pd.merge(df_lagged, shifted_df.add_suffix(f' {shift:+d}min'), left_index=True, right_index=True, how='left')\n",
    "    \n",
    "for shift in flow_shifts:\n",
    "    shifted_df = df.filter(regex='Flow').shift(shift, freq='min')\n",
    "    df_lagged = pd.merge(df_lagged, shifted_df.add_suffix(f' {shift:+d}min'), left_index=True, right_index=True, how='left')\n",
    "\n",
    "for shift in rainfall_min_shifts:\n",
    "    shifted_df = df.filter(regex='Rainfall').shift(shift, freq='min')\n",
    "    df_lagged = pd.merge(df_lagged, shifted_df.add_suffix(f' {shift:+d}min'), left_index=True, right_index=True, how='left')\n",
    "    \n",
    "for shift in rainfall_hour_shifts:\n",
    "    shifted_df = df_rainfall_hourly.shift(shift, freq='h')\n",
    "    df_lagged = pd.merge(df_lagged, shifted_df.add_suffix(f' {shift:+d}h'), left_index=True, right_index=True, how='left')\n",
    "    \n",
    "for shift in rainfall_six_hour_shifts:\n",
    "    shifted_df = df_rainfall_six_hourly.shift(shift, freq='h')\n",
    "    df_lagged = pd.merge(df_lagged, shifted_df.add_suffix(f' {shift:+d}h'), left_index=True, right_index=True, how='left')\n",
    "        \n",
    "for shift in rainfall_day_shifts:\n",
    "    shifted_df = df_rainfall_daily.shift(shift, freq='d')\n",
    "    df_lagged = pd.merge(df_lagged, shifted_df.add_suffix(f' {shift:+d}d'), left_index=True, right_index=True, how='left')\n",
    "    \n",
    "df_lagged = df_lagged.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 590641 entries, 2007-01-01 12:00:00 to 2023-11-06 00:00:00\n",
      "Freq: 15T\n",
      "Columns: 324 entries, Level Chester Le Street to Rainfall Tunstall -7d\n",
      "dtypes: float16(324)\n",
      "memory usage: 369.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_lagged = df_lagged.dropna()\n",
    "df_lagged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\river-level\\Lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "df_lagged.to_feather('data/river_wear_lagged.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('COMP2271_DS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e118ba02d0d63f0776027b45aa6c2794efdfedf6985533f6e5e4217ae01154a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

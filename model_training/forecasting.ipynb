{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from time import sleep\n",
    "from io import StringIO\n",
    "import requests\n",
    "from requests.compat import urljoin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "class MeasureType(Enum):\n",
    "  LEVEL = \"level\"\n",
    "  FLOW = \"flow\"\n",
    "  RAINFALL = \"rainfall\"\n",
    "  \n",
    "  @property\n",
    "  def units(self):  \n",
    "     return {\n",
    "        MeasureType.LEVEL: \"i-900-m-qualified\",\n",
    "        MeasureType.FLOW: \"i-900-m3s-qualified\",\n",
    "        MeasureType.RAINFALL: \"t-900-mm-qualified\",\n",
    "    }.get(self, None)\n",
    "     \n",
    "  @property\n",
    "  def observed_property_name(self):\n",
    "    return {\n",
    "      MeasureType.LEVEL: \"waterLevel\",\n",
    "      MeasureType.FLOW: \"waterFlow\",\n",
    "      MeasureType.RAINFALL: \"rainfall\",\n",
    "  }.get(self, None)\n",
    "\n",
    "class Measure:\n",
    "  def __init__(\n",
    "    self,\n",
    "    station_id: str,\n",
    "    measure_type: MeasureType,\n",
    "  ):\n",
    "    self.station_id = station_id\n",
    "    self.measure_type = measure_type\n",
    "    \n",
    "  def __str__(self):\n",
    "    return f\"{self.station_id}-{self.measure_type.value}-{self.measure_type.units}\"\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return f\"Measure(station_id={self.station_id}, measure_type={self.measure_type})\"\n",
    "    \n",
    "  @staticmethod\n",
    "  def from_string(str: str):\n",
    "    # Format of string is http://environment.data.gov.uk/hydrology/id/measures/ba3f8598-e654-430d-9bb8-e1652e6ff93d-level-i-900-m-qualified\n",
    "    repr = str.split(\"/\")[-1]\n",
    "    station_id_regex = r\"[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\"\n",
    "    possible_measures = {f'{m.value}-{m.units}': m for m in MeasureType}\n",
    "    \n",
    "    station_id = re.search(station_id_regex, repr)\n",
    "    assert station_id, f\"Could not find station ID in {repr}\"\n",
    "    station_id = station_id.group(0)\n",
    "    \n",
    "    measure_type = next((possible_measures[m] for m in possible_measures if m in repr), None)\n",
    "    assert measure_type, f\"Could not find measure type in {repr}\"\n",
    "    \n",
    "    return Measure(station_id, MeasureType(measure_type))\n",
    "    \n",
    "    \n",
    "class HydrologyApi:\n",
    "    API_BASE_URL = \"https://environment.data.gov.uk/hydrology/\"\n",
    "    DATA_DIR = \"data\"\n",
    "    CACHE_DIR = \"cache\"\n",
    "    START_DATE = datetime(2008, 1, 1)\n",
    "    \n",
    "    def _batch_request(\n",
    "      self,\n",
    "      *args,\n",
    "      **kwargs,\n",
    "    ) -> pd.DataFrame:\n",
    "      \"\"\"Deal with batch requests from the API. These may be queued for some time before returning data.\n",
    "\n",
    "      Returns:\n",
    "          pd.DataFrame: The data returned by the API\n",
    "      \"\"\"\n",
    "      class BatchRequestStatus(Enum):\n",
    "        PENDING = \"pending\"\n",
    "        IN_PROGRESS = \"inprogress\"\n",
    "        COMPLETE = \"complete\"\n",
    "        FAILED = \"failed\"\n",
    "        \n",
    "        @staticmethod\n",
    "        def from_string(s: str):\n",
    "          s = s.lower()\n",
    "          s = \"complete\" if s == \"completed\" else s\n",
    "          \n",
    "          assert s in [e.value for e in BatchRequestStatus], f\"Unknown response status: {s}\"\n",
    "          return BatchRequestStatus(s)\n",
    "        \n",
    "      status = BatchRequestStatus.PENDING\n",
    "      \n",
    "      required_headers = {\"Accept-Encoding\": \"gzip\"}\n",
    "      kwargs[\"headers\"] = {**kwargs.get(\"headers\", {}), **required_headers}\n",
    "      \n",
    "      \n",
    "      while status in [BatchRequestStatus.PENDING, BatchRequestStatus.IN_PROGRESS]:\n",
    "        response = requests.get(*args, **kwargs)\n",
    "        \n",
    "        content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "        \n",
    "        if content_type == \"text/csv\":\n",
    "          buffer = StringIO(response.text)\n",
    "          # write buffer to file for debugging\n",
    "          with open(\"data.csv\", \"w\") as f:\n",
    "            f.write(response.text)\n",
    "          return pd.read_csv(buffer, low_memory=False)\n",
    "        \n",
    "        assert \"application/json\" in content_type, f\"Unexpected content type: {content_type}\"\n",
    "        \n",
    "        response_data: dict = response.json()\n",
    "        assert \"status\" in response_data, \"No status field in response\"\n",
    "        status = BatchRequestStatus.from_string(response_data[\"status\"])\n",
    "        \n",
    "        match status:\n",
    "          case BatchRequestStatus.PENDING | BatchRequestStatus.IN_PROGRESS:\n",
    "            eta = response_data.get(\"eta\", 60 * 1000) / 1000\n",
    "            sleep(max(eta*0.1, 1))\n",
    "          \n",
    "          case BatchRequestStatus.COMPLETE:\n",
    "            keys = [\"dataUrl\", \"url\"] # Some responses have dataUrl, some have url\n",
    "            data_url = next((response_data.get(k) for k in keys if k in response_data), None)\n",
    "            assert data_url, f\"Could not find data URL in response: {response_data}\"\n",
    "            return pd.read_csv(data_url)\n",
    "\n",
    "          case BatchRequestStatus.FAILED:\n",
    "            raise Exception(f\"Batch request failed: {response_data}\")\n",
    "          \n",
    "          case _:\n",
    "            raise Exception(f\"Unknown status: {status}\")\n",
    "          \n",
    "    def _request(\n",
    "      self,\n",
    "      *args,\n",
    "      **kwargs,\n",
    "    ):\n",
    "      response = requests.get(*args, **kwargs)\n",
    "      response.raise_for_status()\n",
    "      \n",
    "      response_data = response.json()\n",
    "      assert \"items\" in response_data, \"No items field in response\"\n",
    "      return pd.DataFrame(response_data[\"items\"])\n",
    "      \n",
    "    def get_stations(\n",
    "      self,\n",
    "      measures: MeasureType | List[MeasureType] | None = None,\n",
    "      river: str = None,\n",
    "      position: Tuple[float, float] = None,\n",
    "      radius: float = None,\n",
    "      limit: int = None,\n",
    "      return_df = False,\n",
    "    ):\n",
    "      if isinstance(measures, MeasureType):\n",
    "        measures = [measures]\n",
    "        \n",
    "      lat, long = position if position else (None, None)\n",
    "        \n",
    "      result = requests.get(\n",
    "        urljoin(self.API_BASE_URL, \"id/stations\"),\n",
    "        params = {\n",
    "          \"observedProperty\": [measure.observed_property_name for measure in measures] if measures else None,\n",
    "          \"riverName\": river, \n",
    "          \"lat\": lat,\n",
    "          \"long\": long,\n",
    "          \"dist\": radius,\n",
    "          \"_limit\": limit,\n",
    "          \"status.label\": \"Active\",\n",
    "        },\n",
    "      )\n",
    "      result_json = result.json()\n",
    "      assert \"items\" in result_json, f\"Unexpected response: {result_json}\"\n",
    "      return (\n",
    "        pd.DataFrame(result_json[\"items\"]) \n",
    "        if return_df \n",
    "        else {\n",
    "          station['notation']: station['label']\n",
    "          for station in result_json[\"items\"]\n",
    "        }\n",
    "      )\n",
    "      \n",
    "    def get_measures(\n",
    "      self,\n",
    "      measures: List[Measure],\n",
    "      station_names: Dict[str, str],\n",
    "      start_date: datetime = START_DATE,\n",
    "    ):\n",
    "      # Estimate how many rows we are going to get back\n",
    "      # Each measure is every 15 mins\n",
    "      estimated_rows = 4 * 24 * (datetime.now() - start_date).days * len(measures)\n",
    "      \n",
    "      params = {\n",
    "        'measure': [str(m) for m in measures],\n",
    "        'mineq-date': start_date.strftime(\"%Y-%m-%d\"),\n",
    "        '_limit': int(estimated_rows * 1.1),\n",
    "      }\n",
    "      \n",
    "      if estimated_rows > 2_000_000:\n",
    "        # We need to use the batch api\n",
    "        df = self._batch_request(\n",
    "          urljoin(self.API_BASE_URL, 'data/batch-readings/batch'),\n",
    "          params = params\n",
    "        )\n",
    "        \n",
    "      else:\n",
    "        df = self._request(\n",
    "          urljoin(self.API_BASE_URL, 'data/readings.json'),\n",
    "          params = params\n",
    "        )\n",
    "        df['measure'] = df['measure'].str.get('@id')\n",
    "        \n",
    "      return (\n",
    "        df\n",
    "        .loc[lambda x: x['quality'].isin(['Good', 'Unchecked', 'Estimated'])]\n",
    "        .assign(\n",
    "          timestamp = lambda x: pd.to_datetime(x['dateTime']),\n",
    "          value = lambda x: pd.to_numeric(x['value'], errors='coerce').astype(np.float32),\n",
    "          series_name = lambda x: (\n",
    "            pd.Categorical(x['measure'])\n",
    "            .map(Measure.from_string, na_action='None')\n",
    "            .map(lambda row: f\"{station_names[row.station_id]} ({row.measure_type.value})\", na_action='None')\n",
    "          )\n",
    "        )\n",
    "        .drop(\n",
    "          columns=['measure', 'id', 'date', 'completeness', 'quality', 'qcode', 'dateTime'], \n",
    "          errors='ignore'\n",
    "        )\n",
    "        .pivot(\n",
    "          index='timestamp',\n",
    "          columns='series_name',\n",
    "          values='value'\n",
    "        )\n",
    "        .resample('15min')\n",
    "        .interpolate(\n",
    "          \"time\",\n",
    "          limit_direction='both',\n",
    "          limit=24 * 4,\n",
    "          fill_value=\"extrapolate\",\n",
    "        )\n",
    "      )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HydrologyApi()\n",
    "level_stations = api.get_stations(MeasureType.LEVEL, river=\"River Wear\")\n",
    "rainfall_stations = api.get_stations(MeasureType.RAINFALL, position=(54.774, -1.558), radius=15)\n",
    "\n",
    "measures = [\n",
    "  Measure(station_id, MeasureType.LEVEL) for station_id in level_stations\n",
    "] + [\n",
    "  Measure(station_id, MeasureType.RAINFALL) for station_id in rainfall_stations\n",
    "]\n",
    "\n",
    "stations = {\n",
    "  **level_stations,\n",
    "  **rainfall_stations,\n",
    "}\n",
    "\n",
    "df = api.get_measures(\n",
    "  measures, \n",
    "  stations, \n",
    "  start_date=datetime(2007, 1, 1)\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.auto import NHITS\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "\n",
    "train_df = (\n",
    "  df\n",
    "  .reset_index()\n",
    "  .rename_axis(None, axis=1)\n",
    "  .rename(columns={\n",
    "    \"Durham New Elvet Bridge (level)\": \"y\",\n",
    "    \"timestamp\": \"ds\",\n",
    "  })\n",
    "  .assign(unique_id = \"River Wear\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss():\n",
    "  return MQLoss(quantiles=[0.5, 0.9, 0.99])\n",
    "\n",
    "models = [\n",
    "  NHITS(\n",
    "    h = 4 * 24, # 1 day\n",
    "    input_size = 3 * 4 * 24, # 3 days\n",
    "    hist_exog_list = train_df.columns.drop([\"ds\", \"unique_id\", \"y\"]).to_list(),\n",
    "    scaler_type = 'robust',\n",
    "    loss=make_loss(),\n",
    "    max_steps=1000,\n",
    "  )\n",
    "]\n",
    "\n",
    "nf = NeuralForecast(\n",
    "  models = models,\n",
    "  freq = '15min',\n",
    ")\n",
    "\n",
    "nf.fit(df=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrology API code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import sleep\n",
    "from io import StringIO\n",
    "import urllib3\n",
    "import ujson as json\n",
    "\n",
    "class Measure(Enum):\n",
    "    LEVEL = 'level'\n",
    "    FLOW = 'flow'\n",
    "    RAINFALL = 'rainfall'\n",
    "\n",
    "class HydrologyApi:\n",
    "    API_BASE_URL = \"https://environment.data.gov.uk/hydrology/\"\n",
    "    DATA_DIR = \"data\"\n",
    "    CACHE_DIR = \"cache\"\n",
    "    \n",
    "    units = {\n",
    "        Measure.LEVEL: 'i-900-m-qualified',\n",
    "        Measure.FLOW: 'i-900-m3s-qualified',\n",
    "        Measure.RAINFALL: 't-900-mm-qualified',\n",
    "    }\n",
    "    \n",
    "    observed_property_names = {\n",
    "        Measure.LEVEL: 'waterLevel',\n",
    "        Measure.FLOW: 'waterFlow',\n",
    "        Measure.RAINFALL: 'rainfall',\n",
    "    }\n",
    "    \n",
    "    def __init__(self, max_threads):\n",
    "        self.http = urllib3.PoolManager(maxsize=max_threads)\n",
    "        self.thread_pool = ThreadPoolExecutor(max_workers=max_threads)\n",
    "    \n",
    "    def get_stations_on_river(self, river: str, measure: Measure, limit=1000):\n",
    "        api_url = self.API_BASE_URL + 'id/stations'\n",
    "        result = self.http.request(\n",
    "            'GET',\n",
    "            api_url,\n",
    "            fields={\n",
    "                'observedProperty': HydrologyApi.observed_property_names[measure],\n",
    "                'riverName': river,\n",
    "                '_limit': limit,\n",
    "                \"status.label\": \"Active\"\n",
    "            }\n",
    "        ).data.decode('utf-8')\n",
    "        data = json.loads(result)\n",
    "        return pd.DataFrame(data['items'])\n",
    "        \n",
    "    def get_stations_close_to_with_measure(self, lat, lon, radius, measure: Measure, limit=100):\n",
    "        api_url = self.API_BASE_URL + 'id/stations'\n",
    "        \n",
    "        result = self.http.request(\n",
    "            'GET',\n",
    "            api_url,\n",
    "            fields={\n",
    "                'observedProperty': measure.value,\n",
    "                'lat': lat,\n",
    "                'long': lon,\n",
    "                'dist': radius,\n",
    "                'status.label':'Active',\n",
    "                '_limit': limit\n",
    "            }\n",
    "        ).data.decode('utf-8')\n",
    "        data = json.loads(result)\n",
    "        return pd.DataFrame(data['items'])\n",
    "    \n",
    "    def get_measure(self, measure: Measure, station_id: str, start=None):\n",
    "        api_url = self.API_BASE_URL + f\"id/measures/{station_id}-{measure.value}-{HydrologyApi.units[measure]}/readings\"\n",
    "        # result = urlopen(api_url).read().decode('utf-8')\n",
    "        result = self.http.request(\n",
    "            'GET',\n",
    "            api_url,\n",
    "            fields={}\n",
    "                | ({\n",
    "                    'mineq-date': start.strftime('%Y-%m-%d')\n",
    "                } if start is not None else {}),\n",
    "        ).data.decode('utf-8')\n",
    "        data = json.loads(result)\n",
    "        return pd.DataFrame(data['items'])\n",
    "    \n",
    "    def _cached_batch_request(self, api_url):\n",
    "        cache_key = api_url.split('?')[1]\n",
    "        cache_key = cache_key.replace('=', '_').replace('&', '_')\n",
    "        filepath = os.path.join(self.CACHE_DIR, f\"{cache_key}.feather\")\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Loading from cache: {api_url}\")\n",
    "            \n",
    "            with open(filepath, 'rb') as f:\n",
    "                return pd.read_feather(f)\n",
    "            \n",
    "        data = self._batch_request(api_url)\n",
    "        \n",
    "        if data is not None:\n",
    "            if not os.path.exists(self.CACHE_DIR):\n",
    "                os.makedirs(self.CACHE_DIR)\n",
    "                \n",
    "            data.to_feather(filepath)\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    def _batch_request(self, api_url):\n",
    "        status = \"Pending\"\n",
    "\n",
    "        while status in (\"Pending\", \"InProgress\"):\n",
    "            print(f\"Making request to: {api_url}\")\n",
    "            \n",
    "            request = self.http.request(\n",
    "                'GET', \n",
    "                api_url, \n",
    "                headers={\n",
    "                    'Accept-Encoding': 'gzip'\n",
    "                }\n",
    "            )\n",
    "            content_type = request.headers['Content-Type']\n",
    "\n",
    "            if content_type == 'text/csv':\n",
    "                if len(request.data) == 0:\n",
    "                    print('Got empty CSV')\n",
    "                    return None\n",
    "                buffer = StringIO(request.data.decode('utf-8'))\n",
    "                return pd.read_csv(buffer, low_memory=False)\n",
    "            \n",
    "            assert content_type in (\n",
    "                'application/json',\n",
    "                'application/json;charset=UTF-8'), f\"Unexpected content type: {content_type}\"\n",
    "\n",
    "            data = json.loads(request.data.decode('utf-8'))\n",
    "            status = data[\"status\"]\n",
    "\n",
    "            if status == \"Pending\":\n",
    "                print(f\"Query is pending\")\n",
    "                pos_in_queue = data[\"positionInQueue\"]\n",
    "                print(f\"Position in queue: {pos_in_queue}\")\n",
    "                eta = data[\"eta\"] / 1000\n",
    "                print(f\"Estimated completion: {eta}\")\n",
    "                sleep(eta * 1.1)\n",
    "\n",
    "            elif status == \"InProgress\":\n",
    "                print(f\"Query in progress\")\n",
    "                eta = data[\"eta\"] / 1000\n",
    "                print(f\"Estimated completion: {eta}\")\n",
    "                sleep(eta * 1.1)\n",
    "\n",
    "            elif status in (\"Complete\", \"Completed\"):\n",
    "                print(f\"Query completed: {data}\")\n",
    "                csv_url = data[\"dataUrl\"] if \"dataUrl\" in data else data[\"url\"]\n",
    "                return pd.read_csv(csv_url)\n",
    "\n",
    "            elif status == \"Failed\":\n",
    "                raise Exception(f\"Query failed, response: {data}\")\n",
    "\n",
    "            else:\n",
    "                raise Exception(f\"Unknown status: {data['status']}\")\n",
    "    \n",
    "    def batch_get_measure(self, measure: Measure, station_id):\n",
    "        try:\n",
    "            api_url = self.API_BASE_URL + \\\n",
    "                f\"data/batch-readings/batch/?measure={station_id}-{measure.value}-{HydrologyApi.units[measure]}&mineq-date=2007-01-01\"\n",
    "                \n",
    "            return self._cached_batch_request(api_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get data for station: {station_id}, {e}\")\n",
    "            return None\n",
    "        \n",
    "    def batch_get_measure_on_river(self, measure: Measure, river):\n",
    "        stations = self.get_stations_on_river(river)\n",
    "        return self.batch_get_measure_from_stations(measure, stations)\n",
    "        \n",
    "    def batch_get_measure_from_stations(self, measure: Measure, stations):\n",
    "        data = pd.DataFrame()\n",
    "        threads = [\n",
    "            self.thread_pool.submit(\n",
    "                self.batch_get_measure, measure, station_id)\n",
    "            for station_id in stations['notation'].values\n",
    "        ]\n",
    "        \n",
    "        for thread, station_name in zip(threads, stations['label'].values):\n",
    "            new_data = thread.result()\n",
    "            if new_data is None:\n",
    "                print(f\"No new data for station: {station_name}\")\n",
    "                continue\n",
    "            new_data = new_data.drop(columns=['measure', 'date', 'qcode', 'completeness'])\n",
    "            new_data['station'] = station_name\n",
    "            new_data['station'] = new_data['station'].astype('category')\n",
    "            new_data['dateTime'] = pd.to_datetime(new_data['dateTime'])\n",
    "            new_data['value'] = new_data['value'].astype(float)\n",
    "            new_data['quality'] = new_data['quality'].astype('category')\n",
    "            data = pd.concat([data, new_data])\n",
    "            data.drop_duplicates(subset=['dateTime', 'station'], inplace=True)\n",
    "        return data\n",
    "        \n",
    "        \n",
    "    def get_filename(self, measure: Measure, river):\n",
    "        return f\"{river.lower().replace(' ', '_')}_{measure.value}_raw.feather\"\n",
    "    \n",
    "    def update_dataframe(self, df: pd.DataFrame, measure: Measure, stations: pd.DataFrame):\n",
    "        for station_name, station_id in stations[['label', 'notation']].values:\n",
    "            \n",
    "            last = df[df['station'] == station_name]['dateTime'].max() if len(df) > 0 else None\n",
    "            \n",
    "            new_measurements = self.get_measure(measure, station_id, last)[['dateTime', 'value', 'quality']]\n",
    "            \n",
    "            new_measurements['station'] = station_name\n",
    "            new_measurements['station'] = new_measurements['station'].astype('category')\n",
    "            new_measurements['dateTime'] = pd.to_datetime(new_measurements['dateTime'])\n",
    "            new_measurements['value'] = new_measurements['value'].astype(float)\n",
    "            \n",
    "            df = pd.concat([df, new_measurements])\n",
    "            \n",
    "        df.drop_duplicates(subset=['dateTime', 'station'], inplace=True)\n",
    "        return df\n",
    "            \n",
    "    def load(self, measure: Measure, stations: pd.DataFrame):\n",
    "        df = self.batch_get_measure_from_stations(measure, stations)\n",
    "        df = self.update_dataframe(df, measure, stations)\n",
    "        return df        \n",
    "        \n",
    "def process_hydrology_data(df):\n",
    "    return df[df['quality'].isin(['Good', 'Unchecked', 'Estimated'])] \\\n",
    "        .pivot(index='dateTime', columns='station', values='value') \\\n",
    "        .resample('15min').interpolate('time', limit_direction='both', limit=24*4, fill_value='extrapolate') \\\n",
    "        .astype(np.float16)\n",
    "\n",
    "api = HydrologyApi(max_threads = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 5 stations: ['Chester Le Street' 'Witton Park' 'Sunderland Bridge' 'Stanhope'\n",
      " 'Durham New Elvet Bridge']\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=e7d8bbb6-5bba-4057-9f49-a299482c3348-level-i-900-m-qualified&mineq-date=2007-01-01\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=05784319-693a-4d75-b29e-32f01a99ee4f-level-i-900-m-qualified&mineq-date=2007-01-01\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=ddedb4d9-b2be-47c1-998d-acbc0ffb124b-level-i-900-m-qualified&mineq-date=2007-01-01\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=b29c481a-5012-40f5-bb0c-f9370be34975-level-i-900-m-qualified&mineq-date=2007-01-01\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=ba3f8598-e654-430d-9bb8-e1652e6ff93d-level-i-900-m-qualified&mineq-date=2007-01-01\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 590967 entries, 2007-01-01 00:00:00 to 2023-11-08 21:30:00\n",
      "Freq: 15T\n",
      "Data columns (total 5 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Chester Le Street        590967 non-null  float16\n",
      " 1   Durham New Elvet Bridge  563826 non-null  float16\n",
      " 2   Stanhope                 501615 non-null  float16\n",
      " 3   Sunderland Bridge        589418 non-null  float16\n",
      " 4   Witton Park              585167 non-null  float16\n",
      "dtypes: float16(5)\n",
      "memory usage: 10.1 MB\n"
     ]
    }
   ],
   "source": [
    "stations = api.get_stations_on_river('River Wear', Measure.LEVEL)\n",
    "print(f\"Loading data for {len(stations)} stations: {stations['label'].values}\")\n",
    "level_df = api.load(Measure.LEVEL, stations)\n",
    "level_df = process_hydrology_data(level_df)\n",
    "level_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Flow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 4 stations: ['Chester Le Street' 'Witton Park' 'Sunderland Bridge' 'Stanhope']\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=05784319-693a-4d75-b29e-32f01a99ee4f-flow-i-900-m3s-qualified&mineq-date=2007-01-01\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=e7d8bbb6-5bba-4057-9f49-a299482c3348-flow-i-900-m3s-qualified&mineq-date=2007-01-01\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=ddedb4d9-b2be-47c1-998d-acbc0ffb124b-flow-i-900-m3s-qualified&mineq-date=2007-01-01\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=b29c481a-5012-40f5-bb0c-f9370be34975-flow-i-900-m3s-qualified&mineq-date=2007-01-01\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 588505 entries, 2007-01-01 00:00:00 to 2023-10-14 06:00:00\n",
      "Freq: 15T\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   Chester Le Street  588505 non-null  float16\n",
      " 1   Stanhope           499151 non-null  float16\n",
      " 2   Sunderland Bridge  586956 non-null  float16\n",
      " 3   Witton Park        582705 non-null  float16\n",
      "dtypes: float16(4)\n",
      "memory usage: 9.0 MB\n"
     ]
    }
   ],
   "source": [
    "stations = api.get_stations_on_river('River Wear', Measure.FLOW)\n",
    "print(f\"Loading data for {len(stations)} stations: {stations['label'].values}\")\n",
    "flow_df = api.load(Measure.FLOW, stations)\n",
    "flow_df = process_hydrology_data(flow_df)\n",
    "flow_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have flow data at New Elvet unfortunatly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Rainfall Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5 rainfall stations: ['Evenwood Gate' 'Harpington Hill Farm' 'Copley' 'Tunstall'\n",
      " 'Darlington Lingfield Way']\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=1dabd12c-1d2e-4765-ae38-a4d5a121928d-rainfall-t-900-mm-qualified&mineq-date=2007-01-01\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=bc34e640-d9ae-4362-8804-25d66ca66e4d-rainfall-t-900-mm-qualified&mineq-date=2007-01-01\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=bf61ce31-b20e-4593-85dc-a083133b12ce-rainfall-t-900-mm-qualified&mineq-date=2007-01-01\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=051f1b2a-6aca-4402-8956-5474ad39b12a-rainfall-t-900-mm-qualified&mineq-date=2007-01-01\n",
      "Loading from cache: https://environment.data.gov.uk/hydrology/data/batch-readings/batch/?measure=a8773476-0fde-40c7-a66b-0901e528e8f2-rainfall-t-900-mm-qualified&mineq-date=2007-01-01\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 590967 entries, 2007-01-01 00:00:00 to 2023-11-08 21:30:00\n",
      "Freq: 15T\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   Copley                    576365 non-null  float16\n",
      " 1   Darlington Lingfield Way  542621 non-null  float16\n",
      " 2   Evenwood Gate             517479 non-null  float16\n",
      " 3   Harpington Hill Farm      523182 non-null  float16\n",
      " 4   Tunstall                  546054 non-null  float16\n",
      "dtypes: float16(5)\n",
      "memory usage: 10.1 MB\n"
     ]
    }
   ],
   "source": [
    "rainfall_stations = api.get_stations_close_to_with_measure(54.66305556, -1.67611111, 15, Measure.RAINFALL, limit=10)\n",
    "bad_stations = ['15202aee-c5fd-404d-9de9-7357174ad10c']\n",
    "rainfall_stations = rainfall_stations[~rainfall_stations['notation'].isin(bad_stations)].head(5)\n",
    "print(f\"Using {len(rainfall_stations)} rainfall stations: {rainfall_stations['label'].values}\")\n",
    "\n",
    "rainfall_df = api.load(Measure.RAINFALL, rainfall_stations)\n",
    "rainfall_df = process_hydrology_data(rainfall_df)\n",
    "rainfall_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\river-level\\Lib\\site-packages\\pandas\\core\\nanops.py:1488: RuntimeWarning: overflow encountered in cast\n",
      "  return count.astype(dtype, copy=False)\n",
      "d:\\miniconda3\\envs\\river-level\\Lib\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    "level_df = level_df.fillna(level_df.mean())\n",
    "flow_df = flow_df.fillna(flow_df.mean())\n",
    "rainfall_df = rainfall_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 590967 entries, 2007-01-01 00:00:00 to 2023-11-08 21:30:00\n",
      "Freq: 15T\n",
      "Data columns (total 14 columns):\n",
      " #   Column                             Non-Null Count   Dtype  \n",
      "---  ------                             --------------   -----  \n",
      " 0   Level Chester Le Street            590967 non-null  float16\n",
      " 1   Level Durham New Elvet Bridge      563826 non-null  float16\n",
      " 2   Level Stanhope                     501615 non-null  float16\n",
      " 3   Level Sunderland Bridge            589418 non-null  float16\n",
      " 4   Level Witton Park                  585167 non-null  float16\n",
      " 5   Flow Chester Le Street             588505 non-null  float16\n",
      " 6   Flow Stanhope                      499151 non-null  float16\n",
      " 7   Flow Sunderland Bridge             586956 non-null  float16\n",
      " 8   Flow Witton Park                   582705 non-null  float16\n",
      " 9   Rainfall Copley                    590967 non-null  float16\n",
      " 10  Rainfall Darlington Lingfield Way  590967 non-null  float16\n",
      " 11  Rainfall Evenwood Gate             590967 non-null  float16\n",
      " 12  Rainfall Harpington Hill Farm      590967 non-null  float16\n",
      " 13  Rainfall Tunstall                  590967 non-null  float16\n",
      "dtypes: float16(14)\n",
      "memory usage: 36.4 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(\n",
    "    level_df.add_prefix('Level '),\n",
    "    flow_df.add_prefix('Flow '),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='outer',\n",
    ")\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    rainfall_df.add_prefix('Rainfall '),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='outer',\n",
    ")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Lag Features\n",
    "for some reason makes loads of nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rainfall_hourly = df.filter(regex='Rainfall').resample('1h').sum()\n",
    "df_rainfall_six_hourly = df.filter(regex='Rainfall').resample('6h').sum()\n",
    "df_rainfall_daily = df.filter(regex='Rainfall').resample('1d').sum()\n",
    "\n",
    "target_cols = ['Level Durham New Elvet Bridge']\n",
    "target_shifts = [-15, -30, -60, -90, -120]\n",
    "level_shifts = [15, 30, 60, 90, 120]\n",
    "flow_shifts = [15, 30, 60, 90, 120]\n",
    "rainfall_min_shifts = [15, 30, 60]\n",
    "rainfall_hour_shifts = [2, 3, 4, 5, 6]\n",
    "rainfall_six_hour_shifts = [12, 18, 24, 30, 36, 42, 48]\n",
    "rainfall_day_shifts = [3, 4, 5, 6, 7]\n",
    "\n",
    "output_target_cols = []\n",
    "\n",
    "df_lagged = df.copy()\n",
    "\n",
    "for shift in target_shifts:\n",
    "    shifted_df = df[target_cols].shift(shift, freq='min')\n",
    "    shifted_df = shifted_df.add_suffix(f' {-shift:+d}min')\n",
    "    output_target_cols.extend(shifted_df.columns)\n",
    "    df_lagged = pd.concat([df_lagged, shifted_df], axis=1)\n",
    "\n",
    "for shift in level_shifts:\n",
    "    shifted_df = df.filter(regex='Level').shift(shift, freq='min')\n",
    "    df_lagged = pd.concat([df_lagged, shifted_df.add_suffix(f' {-shift:+d}min')], axis=1)\n",
    "    \n",
    "for shift in flow_shifts:\n",
    "    shifted_df = df.filter(regex='Flow').shift(shift, freq='min')\n",
    "    df_lagged = pd.concat([df_lagged, shifted_df.add_suffix(f' {-shift:+d}min')], axis=1)\n",
    "\n",
    "for shift in rainfall_min_shifts:\n",
    "    shifted_df = df.filter(regex='Rainfall').shift(shift, freq='min')\n",
    "    df_lagged = pd.concat([df_lagged, shifted_df.add_suffix(f' {-shift:+d}min')], axis=1)\n",
    "    \n",
    "for shift in rainfall_hour_shifts:\n",
    "    shifted_df = df_rainfall_hourly.shift(shift, freq='h')\n",
    "    df_lagged = pd.concat([df_lagged, shifted_df.add_suffix(f' {-shift:+d}h')], axis=1)\n",
    "    \n",
    "for shift in rainfall_six_hour_shifts:\n",
    "    shifted_df = df_rainfall_six_hourly.shift(shift, freq='h')\n",
    "    df_lagged = pd.concat([df_lagged, shifted_df.add_suffix(f' {-shift:+d}h')], axis=1)\n",
    "    \n",
    "for shift in rainfall_day_shifts:\n",
    "    shifted_df = df_rainfall_daily.shift(shift, freq='d')\n",
    "    df_lagged = pd.concat([df_lagged, shifted_df.add_suffix(f' {-shift:+d}d')], axis=1)\n",
    "    \n",
    "df_lagged = df_lagged.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\river-level\\Lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "df_lagged.to_feather('data/river_wear_lagged.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('COMP2271_DS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e118ba02d0d63f0776027b45aa6c2794efdfedf6985533f6e5e4217ae01154a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
